% Chapter Template

\chapter{Matrix Factorization Models} % Main chapter title

\label{Matrix Factorization Models} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 4 \emph{Matrix Factorization Models}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

In Chapter 3, we exposed in details the $DLMR$ model and talked about $LMR$, $pLSI$ and $LDA$ which are all based on a probabilistic modeling of the smartphone logs. However, the probabilistic approach in not the only way to tackle the problem. To have a complete overview, we present in this chapter models based on a matrix factorization approach that can extract common latent characteristics (i.e behaviors) from the smartphone logs. First we introduce $Singular$ $Value$ $Decomposition$ ($SVD$) \cite{svd} technique. Then, second we talk about $Linearly$ $Constrained$ $Bayesian$ $Matrix$ $Factorization$ ($LCBMF$) \cite{lcbmf}.
%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Singular Value Decomposition (SVD)}

$Singular$ $Value$ $Decomposition$ ($SVD$) \cite{svd} is a common technique for analysis of data and has a wide range of applications. It is for example used in $Latent Semantic Indexing$ ($LSI$) \cite{lsi}, the ancestor of $pLSI$. We briefly present in this section how $SVD$ is used to extract behaviors from smartphone logs.


%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{matrix representation of the smartphone logs} \label{4.1.1}

In this section, the corpus of smartphone logs $R$ in represented as a matrix $\boldsymbol{X}$ of $I$ rows and $M$ columns. Each column vector represents a record $\mathbf{r}_{m}$. In this section $\mathbf{r}_{m}$ is a vector of the size of the language defined by $R$, $I=\sum_{f=1}^{J}I_{f}$ where each dimension represents one possible realization.  In this part, each dimension contains the number of times the corresponding realization is observed. \par

$SVD$ finds an approximation $\boldsymbol{\widehat{X}}$ of $\boldsymbol{X}$ that minimizes the Frobenius norm $\left \|  \boldsymbol{X}-\boldsymbol{\widehat{X}}\right \|^{2}$. $\boldsymbol{\widehat{X}}$ is expressed as the product of three matrixes $\boldsymbol{U}$ of dimension $I\times K$, $\boldsymbol{S}$ $K \times K$ and $\boldsymbol{V}$ $\in\mathbb{R}^{K\times M}$, such that $\boldsymbol{S}$ is a diagonal matrix, $\boldsymbol{U}$ and $\boldsymbol{V}$ are orthogonal matrixes.
\\The tree factorizing matrixes can be interpreted as follows. Each of the $K$ columns of $\boldsymbol{U}$ could be interpreted as a behavior, where each dimension of the column vector (of size $I$) contains a value indicating how much the corresponding realization is correlated (positively or negatively) with the given behavior. The diagonal vector of matrix $\boldsymbol{S}$ (of size $K$) contains by definition positive values. They represent the importance of each behavior in the matrix $X$. Finally, each of the $M$ columns of  $\boldsymbol{U}$ could be seen as the behaviors importance in each record, where each dimension of the $m^{th}$ column vector (of size $K$) contains a value indicating how much each behavior is important to describe the record $\mathbf{r}_{m}$.

%-----------------------------------
%	SUBSECTION 2
%-----------------------------------
\subsection{TF-IDF transformation}
As described in \ref{4.1.1} $SVD$ objective function is to minimize the absolute distance $\left \|  \boldsymbol{X}-\boldsymbol{\widehat{X}}\right \|^{2}$. This implies that the dimensions $i\in\{1,...,I\}$ (i.e realizations) that are the most recurrent in $\boldsymbol{X}$ will catch the most the attention of $SVD$ in minimizing its objective function. Thus the most recurrent realizations will get the most importance in the task of finding $\boldsymbol{\widehat{X}}$. While this may seems a good thing at first look (one could argue that the most recurrent realizations should get the most importance because they are the most representative of $X$), it is not true that the most recurrent realizations are the most representative of the data. Indeed, if $Bob$ is always at home and never leaves it, then knowing that Bob is at home does not help in describing current Bob behavior (as it is always the case). Indeed, the very frequent realizations are not carrying meaning and for this reason they should not concentrate all the importance of $SVD$. This problem is also faced in document corpus when using $SVD$ (in $LSI$). \par

To face this problem, we use the $Term$ $Frequency$ $Inverse$ $Document$ $Frequency$ transformation ($TF\_IDF$) \cite{tfidf} on the matrix $\boldsymbol{X}$. This transformation applies a weighting to the different dimensions of $\boldsymbol{X}$ so that it gives more importance to the occurrences that are the most meaningful in describing the data. Each occurrence in $\boldsymbol{X}$ is wighted according to to two criteria: its importance inside the record and its importance inside the corpus. It translates the following intuitions: A realization that is frequent is a record is important in describing this record. A realization that is rare in the corpus carries meaning when it is present in a record.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Linearly Constrained Bayesian Matrix Factorization (LCBMF)}

While $SVD$ provides nice and easy ways in factorizing matrices and expressing statistical dependence of data, it suffers from non desirable properties. First, $\boldsymbol{U}$ and $\boldsymbol{V}$ contains both positive and negative values which make them complicated and not intuitive to interpret. Second, there is not an objective measurement indicating the quality of the resulting factorization (estimation). $Bayesian$ $Non$ $Negative$ $Matrix$ $Factorization$ \cite{bnmf} came as a response to these problems. In fact, they factorize a matrix $\boldsymbol{X}$ in matrixes containing only positive values and provide uncertainty measures of the factorizations. 
\\$Linearly$ $Constrained$ $Bayesian$ $Matrix$ $Factorization$ ($LCBMF$) is bayesian matrix factorization that provides the possibility to impose any kind of linear constrains to the factorizing matrixes \cite{lcbmf}. It factorizes a matrix $\boldsymbol{X}$ into two matrixes $\boldsymbol{A}$ and $\boldsymbol{B}$ where the elements of $\boldsymbol{A}$ and $\boldsymbol{B}$ are subject to some equality and inequality constrains. To show the benefits and the large degree of freedom that $LCBMF$ brings, we start by exposing some applications of $LCBMF$. Then, we explain the constrains chosen for our problem.
 
%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{LCBMF applications and examples}

$LCBMF$ is evaluated in \cite{bnmf} for blind source separation. Handwritten digits grayscale images are superposed randomly to create a dataset of mixed images (each image is a mix of two hand written digits). The task is then to see how well can the sources that generated those images (i.e the handwritten digits) can be recovered. Ideally, one would expect to find nine separate sources where each source represents a different digit. Noting the dataset of mixed images as a matrix $\boldsymbol{X}$ where each column vector represents a mixed image, $\boldsymbol{X}$ $\in\mathbb{R}^{I\times M}$ expressed as the factor of two matrixes $\boldsymbol{A}$ $\in\mathbb{R}^{I\times K}$ and $\boldsymbol{B}$ $\in\mathbb{R}^{K\times M}$ where $K$ is the number of the target hidden sources. By imposing that each column vector in $\boldsymbol{B}$ sums to $1$ and each element in $\boldsymbol{A}$ to be between $0$ and $1$ (note that input grayscale images represent intensities of gray that are between $0$ and $1$), we force the original matrix $\boldsymbol{X}$ to be expressed as the sum of original grayscale sources ($\boldsymbol{A}$) mixed with different proportions (coefficients $\boldsymbol{B}$) (Indeed, each original element $x_{im}$ is expressed as the sum of $x_{im}=\sum_{k=1}^{K}a_{ik}b_{km}$). The linear constrains imposed to $\boldsymbol{A}$ and $\boldsymbol{B}$ allowed $LCBMF$ to adapt to the specific problem of finding handwritten sources digits from mixed images. The results in \cite{bnmf} shows that $LCBMF$ was successful to recover the original digits which is not the case of the other factorizing approaches. \par

in \cite{superbehaviors}, Ma et al. wanted to extract common behaviors of users based on their smartphone logs. They had a matrix $\boldsymbol{X}$ $\in\mathbb{R}^{I\times M}$ where the each row represents a type of behavior and each column represent a user.  The column $m^{th}$ column $\boldsymbol{x}^{m}$ represents the behaviors of the user $m$ where each dimension $i$ contains the number of times the behavior $i$ was observed in the user $m$. To find common users' behaviors (called super-behaviors) from this matrix, they assumed that each user act as a mixture of super-behaviors, where each super behavior is represented as a vector of size $I$ containing the importance of each simple behavior. Thus, they used $LCBMF$ to decompose $\boldsymbol{X}$ into $\boldsymbol{A}$ $\in\mathbb{R}^{I\times M}$ and $\boldsymbol{B}$ $\in\mathbb{R}^{K\times M}$, where  $k^{th}$ column of $\boldsymbol{A}$ represents the super-behavior $k$ and the $m^{th}$ column of $\boldsymbol{B}$ represents the mixture coefficients of the super-behaviors for the user $m$. For this, they imposed that each column vector in $\boldsymbol{B}$ sums to $1$ (representing mixture over super-behaviors) and element in $\boldsymbol{A}$ to be positive. \par

Those two examples show the ability of $LCBMF$ to adapt to different problems and expose the large degree of freedom provided by the ability to fix linear constrains. In the next part, we show the linear constrains that we used to answer our problem.

%-----------------------------------
%	SUBSECTION 2
%-----------------------------------
\subsection{Linear constrains for smartphone logs matrix} \label{4.2}

In this part, we represent smartphone logs as a matrix $\boldsymbol{X}$ of $I$ rows and $M$ columns. As in \ref{4.1.1}, each column vector represents a record $\mathbf{r}_{m}$. Moreover, $\mathbf{r}_{m}$ is also a vector of the size of the language ($I=\sum_{f=1}^{J}I_{f}$) defined by $R$ where each dimension represents one possible realization. However, in this part, each dimension contains the number of observation of a given realization $(f,v)$ normalized by the total number of realizations observed for feature $f$. This means that in each records, the values attributed to the different realizations belonging to the same feature sum to $1$. For example, if Bob opened $2$ times his preferred news application and $1$ time his e-mail application (during the time frame of a record observation) then the realization corresponding to ($"application$ $launch"$, $"news$ $app"$) would get the score of $\frac{2}{3}$ and the realization corresponding to ($"application$ $launch"$, $"e-mail$ $app"$) would get the score of $\frac{1}{3}$. The realizations corresponding to the other application launches would get the score of $0$. By doing this, the records would represent the probabilities of the user's actions. \par

Having $\boldsymbol{X}$, the goal is to extract the user's behaviors. Thus, we use $LCBMF$ and we impose the matrix $\boldsymbol{A}$ $\in\mathbb{R}^{I\times K}$ to represent the behaviors and the matrix $\boldsymbol{B}$ $\in\mathbb{R}^{K\times M}$ to represent the mixture coefficients of the behaviors in each record. We do this by setting each column vector $\mathbf{b}_{m}$ in $\boldsymbol{B}$ sums to $1$ (to represent the coefficients of behaviors in record $m$). Concerning the columns of $\boldsymbol{A}$, we impose the values attributed to the realizations of the same feature to sum to $1$ (the same as the columns of the original matrix). Thus, an observed record $\mathbf{r}_{m}$ would be obtained by the mixture of the different behaviors represented by the columns $\{\mathbf{a}_{k}\}_{k=\{1,...,K\}}$ of matrix $\boldsymbol{A}$. Indeed a record vector $\mathbf{r}_m$ is expressed as by $\mathbf{r}_m = \sum_{k=1}^{K}b_{k,m}\mathbf{a}_{k}$. \par

This concludes the chapter about matrix factorization models. In this chapter we presented two different matrix factorization techniques that are candidate to solve our problem. We talked about $SVD$ a common and widely used matrix factorization technique and $LCBMF$, a sophisticated and powerful technique that we adapted according to our needs.